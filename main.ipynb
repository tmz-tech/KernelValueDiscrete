{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91090a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.stats.sandwich_covariance import cov_hac\n",
    "from joblib import Parallel, delayed\n",
    "import scipy.stats as stats\n",
    "from statsmodels.stats import power\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "\n",
    "from nonparametric import *\n",
    "from basis import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4030e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scaling:\n",
    "    \"\"\"\n",
    "    A class used to scale data to the range [-1, 1] and inverse scale it back to the original range.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    scale_to_minus_one_to_one(column)\n",
    "        Scales a single column to the range [-1, 1].\n",
    "\n",
    "    scale_to_minus_one_to_one_df(df)\n",
    "        Scales all columns in a DataFrame to the range [-1, 1].\n",
    "\n",
    "    inverse_scale(column, min_val, max_val)\n",
    "        Inverse scales a single column back to its original range.\n",
    "\n",
    "    inverse_scale_df(scaled_df)\n",
    "        Inverse scales all columns in a scaled DataFrame back to their original ranges.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Scaling class with attributes to store scaled data and min-max values.\n",
    "        \"\"\"\n",
    "        # No initialization of variables is strictly needed here, but we define the __init__ method for potential future extensions.\n",
    "        return None\n",
    "    \n",
    "    def scale_to_minus_one_to_one(self, column):\n",
    "        \"\"\"\n",
    "        Scales a single column to the range [-1, 1] using min-max normalization.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column : pandas Series\n",
    "            The column to scale.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scaled_column : pandas Series\n",
    "            The scaled column with values in the range [-1, 1].\n",
    "        min_val : float\n",
    "            The minimum value of the original column.\n",
    "        max_val : float\n",
    "            The maximum value of the original column.\n",
    "        \"\"\"\n",
    "        # Find the minimum and maximum values in the column\n",
    "        min_val = column.min()\n",
    "        max_val = column.max()\n",
    "        # Scale the column to the range [-1, 1]\n",
    "        scaled_column = 2 * (column - min_val) / (max_val - min_val) - 1\n",
    "        # Return the scaled column along with its original min and max values\n",
    "        return scaled_column, min_val, max_val\n",
    "    \n",
    "    def scale_to_minus_one_to_one_df(self, df):\n",
    "        \"\"\"\n",
    "        Scales all columns in a DataFrame to the range [-1, 1] using min-max normalization.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas DataFrame\n",
    "            The DataFrame to scale.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.scaled_df : pandas DataFrame\n",
    "            A new DataFrame with all columns scaled to the range [-1, 1].\n",
    "        \"\"\"\n",
    "        # Initialize an empty DataFrame to store the scaled data\n",
    "        self.scaled_df = pd.DataFrame()\n",
    "        # Initialize a dictionary to store the min and max values for each column\n",
    "        self.min_max_values = {}\n",
    "        \n",
    "        # Iterate through each column in the DataFrame\n",
    "        for column in df.columns:\n",
    "            # Scale the column and retrieve its min and max values     \n",
    "            scaled_column, min_val, max_val = self.scale_to_minus_one_to_one(df[column])\n",
    "            # Add the scaled column to the new DataFrame\n",
    "            self.scaled_df[column] = scaled_column\n",
    "            # Store the min and max values for this column\n",
    "            self.min_max_values[column] = (min_val, max_val)\n",
    "            \n",
    "        # Return the DataFrame with all columns scaled to [-1, 1]   \n",
    "        return self.scaled_df\n",
    "    \n",
    "    def inverse_scale(self, column, min_val, max_val):\n",
    "        \"\"\"\n",
    "        Inverse scales a single column from the range [-1, 1] back to its original range.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column : pandas Series\n",
    "            The scaled column to inverse scale.\n",
    "        min_val : float\n",
    "            The minimum value of the original column before scaling.\n",
    "        max_val : float\n",
    "            The maximum value of the original column before scaling.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        original_column : pandas Series\n",
    "            The column rescaled back to its original range.\n",
    "        \"\"\"\n",
    "        # Reverse the scaling process to return the column to its original range\n",
    "        original_column = (column + 1) / 2 * (max_val - min_val) + min_val\n",
    "        # Return the rescaled column\n",
    "        return original_column\n",
    "    \n",
    "    def inverse_scale_df(self, scaled_df):\n",
    "        \"\"\"\n",
    "        Inverse scales all columns in a DataFrame from the range [-1, 1] back to their original ranges.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scaled_df : pandas DataFrame\n",
    "            The DataFrame with scaled columns to inverse scale.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self.original_df : pandas DataFrame\n",
    "            A new DataFrame with all columns rescaled back to their original ranges.\n",
    "        \"\"\"\n",
    "        # Initialize an empty DataFrame to store the original data\n",
    "        self.original_df = pd.DataFrame()\n",
    "        \n",
    "        # Iterate through each column in the scaled DataFrame\n",
    "        for column in scaled_df.columns:\n",
    "            # Retrieve the original min and max values for this column\n",
    "            min_val, max_val = self.min_max_values[column]\n",
    "            # Inverse scale the column and add it to the new DataFrame\n",
    "            self.original_df[column] = self.inverse_scale(scaled_df[column], min_val, max_val)\n",
    "            \n",
    "        # Return the DataFrame with all columns rescaled to their original ranges\n",
    "        return self.original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83f94775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scalingDataFrame:\n",
    "    \"\"\"\n",
    "    A class used to scale a DataFrame's entire range to [-1, 1] and inverse scale it back to the original range.\n",
    "    \n",
    "    Methods\n",
    "    -------\n",
    "    scale_to_minus_one_to_one_df(df)\n",
    "        Scales all values in a DataFrame to the range [-1, 1].\n",
    "    \n",
    "    inverse_scale_df(scaled_df)\n",
    "        Inverse scales a scaled DataFrame back to its original range.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Scaling class with attributes to store the original min and max values for the entire DataFrame.\n",
    "        \"\"\"\n",
    "        # These will store the overall min and max values for the entire DataFrame\n",
    "        self.min_val = None\n",
    "        self.max_val = None\n",
    "    \n",
    "    def scale_to_minus_one_to_one_df(self, df):\n",
    "        \"\"\"\n",
    "        Scales the entire DataFrame to the range [-1, 1] based on the global min and max of all values in the DataFrame.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pandas DataFrame\n",
    "            The DataFrame to scale.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        scaled_df : pandas DataFrame\n",
    "            A new DataFrame with all values scaled to the range [-1, 1].\n",
    "        \"\"\"\n",
    "        # Find the overall min and max values for the entire DataFrame\n",
    "        self.min_val = df.min().min()\n",
    "        self.max_val = df.max().max()\n",
    "        \n",
    "        # Scale the entire DataFrame to the range [-1, 1]\n",
    "        scaled_df = 2 * (df - self.min_val) / (self.max_val - self.min_val) - 1\n",
    "        \n",
    "        # Return the scaled DataFrame\n",
    "        return scaled_df\n",
    "    \n",
    "    def inverse_scale_df(self, scaled_df):\n",
    "        \"\"\"\n",
    "        Inverse scales a DataFrame from the range [-1, 1] back to its original range.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scaled_df : pandas DataFrame\n",
    "            The DataFrame with scaled values to inverse scale.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        original_df : pandas DataFrame\n",
    "            A new DataFrame with all values rescaled back to their original range.\n",
    "        \"\"\"\n",
    "        # Reverse the scaling process to return the DataFrame to its original range\n",
    "        original_df = (scaled_df + 1) / 2 * (self.max_val - self.min_val) + self.min_val\n",
    "        \n",
    "        # Return the rescaled DataFrame\n",
    "        return original_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceab2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculateTheta:\n",
    "    \"\"\"\n",
    "    Class to calculate theta values using policy estimation, reward estimation, and basis function expansion.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_df : DataFrame\n",
    "        DataFrame containing the state values.\n",
    "    action_df : DataFrame\n",
    "        DataFrame containing the action values.\n",
    "    reward_df : DataFrame\n",
    "        DataFrame containing the reward values.\n",
    "    next_x_df : DataFrame\n",
    "        DataFrame containing the next state values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_df, action_df, reward_df, next_x_df):\n",
    "        # Initialize dataframes for state, action, reward, and next state\n",
    "        self.state_df = state_df\n",
    "        self.action_df = action_df\n",
    "        self.reward_df = reward_df\n",
    "        self.next_x_df = next_x_df\n",
    "        \n",
    "        # Scale the state and next state dataframes to the range [-1, 1]\n",
    "        \n",
    "        state_next_state_df = pd.concat([self.state_df, self.next_x_df], axis=1)\n",
    "        self.sc = scalingDataFrame()#scaling()\n",
    "        self.scaled_state_next_state_df = self.sc.scale_to_minus_one_to_one_df(state_next_state_df)\n",
    "        \n",
    "        self.scaled_state_df = self.scaled_state_next_state_df.iloc[:,:-1]\n",
    "        self.scaled_next_x_df = pd.DataFrame(self.scaled_state_next_state_df.iloc[:,-1])\n",
    "        \n",
    "        #self.sc_state = scaling()\n",
    "        #self.scaled_state_df = self.sc_state.scale_to_minus_one_to_one_df(self.state_df)\n",
    "        \n",
    "        #self.sc_next_x = scaling()\n",
    "        #self.scaled_next_x_df = self.sc_next_x.scale_to_minus_one_to_one_df(self.next_x_df)\n",
    "        \n",
    "        self.sc_reward = scaling()\n",
    "        self.scaled_reward_df = self.sc_reward.scale_to_minus_one_to_one_df(self.reward_df)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self, order: int = 3, search_interval=np.linspace(0.05, 0.06, 100), cv_k: int = 20):\n",
    "        \"\"\"\n",
    "        Fit the model by searching for optimal bandwidth and computing policy, reward, and basis.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        search_interval : array-like, optional\n",
    "            Interval to search for the optimal bandwidth for kernel density estimation.\n",
    "        cv_k : int, optional\n",
    "            Number of folds for cross-validation in the search for the optimal bandwidth.\n",
    "        order : int, optional\n",
    "            The order of the Chebyshev basis function expansion.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        int : 0\n",
    "            Always returns 0 after fitting.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Search for the optimal bandwidth for kernel density estimation\n",
    "        self.h_x = self.search_optimal_bandwidth(self.scaled_state_df, search_interval, cv_k)\n",
    "        \n",
    "        # Estimate policy and compute policy data\n",
    "        self.est_pi_data_all, self.est_pi_actual = self.estimation_policy(self.h_x, self.scaled_state_df, self.action_df)\n",
    "        \n",
    "        # Estimate the reward\n",
    "        self.r_pi_est = self.estimation_reward(self.h_x, self.scaled_state_df, self.scaled_reward_df)\n",
    "        \n",
    "        # Compute the basis functions\n",
    "        self.basis_df, self.basis_dict, self.hat_psi_next = self.compute_basis(self.h_x, self.scaled_state_df, self.scaled_next_x_df, order)\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def calculate_theta_w(self, w: int = 10, gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Calculate theta_w based on time lag `w` and discount factor `gamma`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        w : int, optional\n",
    "            The time lag used in the reward estimation.\n",
    "        gamma : float, optional\n",
    "            The discount factor for future rewards.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            DataFrame of theta estimates.\n",
    "        DataFrame\n",
    "            DataFrame of basis functions (adjusted for lag).\n",
    "        dict\n",
    "            Dictionary of basis function definitions.\n",
    "        DataFrame\n",
    "            Estimated policy data for all actions (adjusted for lag).\n",
    "        DataFrame\n",
    "            Estimated actual policy data (adjusted for lag).\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate reward with time lag `w`\n",
    "        self.r_pi_w_est = self.estimation_reward_w(self.h_x, self.scaled_state_df, self.scaled_reward_df, w)\n",
    "        \n",
    "        # Compute discounted reward difference\n",
    "        R_pi = - self.r_pi_est[:-w] + (gamma**w) * self.r_pi_w_est[:-w]\n",
    "        \n",
    "        # Compute zeta_w (difference between next basis and current basis, scaled by gamma)\n",
    "        self.zeta_w = (gamma * self.hat_psi_next - self.basis_df.values)[:-w]\n",
    "        \n",
    "        # Cross-products of zeta_w vectors\n",
    "        zeta_cross = np.array([z @ z.T for z in self.zeta_w])\n",
    "        \n",
    "        # Element-wise product of zeta_w and reward difference\n",
    "        zeta_R_pi = self.zeta_w * R_pi[:, np.newaxis]\n",
    "        \n",
    "        # Compute theta_hat_w by dividing zeta_R_pi by the cross-products\n",
    "        self.theta_hat_w = zeta_R_pi / zeta_cross.reshape(-1, 1)\n",
    "        \n",
    "        # Store the results in a DataFrame\n",
    "        self.theta_hat_w_df = pd.DataFrame(self.theta_hat_w, columns=self.basis_df.columns)\n",
    "        \n",
    "        return self.theta_hat_w_df, self.basis_df[:-w], self.basis_dict, self.est_pi_data_all[:-w], self.est_pi_actual[:-w]\n",
    "    \n",
    "    def calculate_theta_inf(self, gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Calculate theta in the infinite-horizon case using the discount factor `gamma`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float, optional\n",
    "            The discount factor for future rewards.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            DataFrame of theta estimates for the infinite-horizon case.\n",
    "        DataFrame\n",
    "            DataFrame of basis functions.\n",
    "        dict\n",
    "            Dictionary of basis function definitions.\n",
    "        DataFrame\n",
    "            Estimated policy data for all actions.\n",
    "        DataFrame\n",
    "            Estimated actual policy data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute the reward estimate without time lag\n",
    "        R_pi = - self.r_pi_est\n",
    "        \n",
    "        # Compute zeta_inf (difference between next basis and current basis, scaled by gamma)\n",
    "        self.zeta_inf = (gamma * self.hat_psi_next - self.basis_df.values)\n",
    "        \n",
    "        # Cross-products of zeta_inf vectors\n",
    "        zeta_cross = np.array([z @ z.T for z in self.zeta_inf])\n",
    "        \n",
    "        # Element-wise product of zeta_inf and reward estimate\n",
    "        zeta_R_pi = self.zeta_inf * R_pi[:, np.newaxis]\n",
    "        \n",
    "        # Compute theta_hat_inf by dividing zeta_R_pi by the cross-products\n",
    "        self.theta_hat_inf = zeta_R_pi / zeta_cross.reshape(-1, 1)\n",
    "        \n",
    "        # Store the results in a DataFrame\n",
    "        self.theta_hat_inf_df = pd.DataFrame(self.theta_hat_inf, columns=self.basis_df.columns)\n",
    "        \n",
    "        return self.theta_hat_inf_df, self.basis_df, self.basis_dict, self.est_pi_data_all, self.est_pi_actual\n",
    "    \n",
    "    def search_optimal_bandwidth(self, scaled_state_df, search_interval=np.linspace(0.05, 0.06, 100), cv_k=20):\n",
    "        \"\"\"\n",
    "        Search for the optimal bandwidth for kernel density estimation using cross-validation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        scaled_state_df : DataFrame\n",
    "            Scaled state data.\n",
    "        search_interval : array-like, optional\n",
    "            Range of bandwidth values to search.\n",
    "        cv_k : int, optional\n",
    "            Number of folds for cross-validation.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The optimal bandwidth value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Perform a grid search for optimal bandwidth using cross-validation\n",
    "        grid_search_custom = GridSearchCV(estimator=KDE(),  \n",
    "                                          param_grid={'bandwidth': search_interval},\n",
    "                                          cv=cv_k)\n",
    "        grid_search_custom.fit(scaled_state_df.values)\n",
    "        h_x = grid_search_custom.best_params_[\"bandwidth\"]\n",
    "        \n",
    "        print(grid_search_custom.best_params_)  # Output the optimal bandwidth\n",
    "        \n",
    "        return h_x\n",
    "    \n",
    "    def extract_est_pi_actual(self, est_pi_data_all, action_df, unique_actions):\n",
    "        \"\"\"\n",
    "        Extract actual policy estimates corresponding to each action.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        est_pi_data_all : array-like, shape (n_samples, n_actions)\n",
    "            Estimated policy data for all actions.\n",
    "        action_df : DataFrame\n",
    "            The action data corresponding to each sample.\n",
    "        unique_actions : array-like\n",
    "            Unique action values.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        array-like, shape (n_samples,)\n",
    "            The actual policy estimates corresponding to the action taken.\n",
    "        \"\"\"\n",
    "        \n",
    "        est_pi_actual = np.zeros(est_pi_data_all.shape[0])\n",
    "        \n",
    "        # For each action, apply a mask to extract the actual policy estimates\n",
    "        for index, action in enumerate(unique_actions):\n",
    "            mask = (action_df.squeeze() == action)  # Mask for each action\n",
    "            est_pi_actual[mask] = est_pi_data_all[mask, index]\n",
    "            \n",
    "        return est_pi_actual\n",
    "    \n",
    "    def estimation_policy(self, h_x, scaled_state_df, action_df):\n",
    "        \"\"\"\n",
    "        Estimate the policy probabilities using kernel density estimation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        h_x : float\n",
    "            The bandwidth for kernel density estimation.\n",
    "        scaled_state_df : DataFrame\n",
    "            Scaled state data.\n",
    "        action_df : DataFrame\n",
    "            Action data corresponding to each state.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        est_pi_data_all : array-like, shape (n_samples, n_actions)\n",
    "            Estimated policy data for all actions.\n",
    "        est_pi_actual : array-like, shape (n_samples,)\n",
    "            Actual policy estimates corresponding to the action taken.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize policy estimator\n",
    "        est_pi = est_policy(h_x)\n",
    "        est_pi.fit(scaled_state_df.values, action_df.values.ravel())\n",
    "        \n",
    "        # Get the unique actions\n",
    "        unique_actions = np.unique(action_df.values)\n",
    "         \n",
    "        # Estimate policy for each action and combine into a single matrix\n",
    "        est_pi_data = [est_pi(scaled_state_df.values, action) for action in unique_actions]\n",
    "        est_pi_data_all = np.concatenate([data.reshape(-1, 1) for data in est_pi_data], axis=1)\n",
    "       \n",
    "        # Extract the actual policy estimates\n",
    "        est_pi_actual = self.extract_est_pi_actual(est_pi_data_all, action_df, unique_actions)\n",
    "        \n",
    "        return est_pi_data_all, est_pi_actual\n",
    "\n",
    "    def estimation_reward(self, h_x, scaled_state_df, reward_df):\n",
    "        \"\"\"\n",
    "        Estimate the expected reward for each state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_x : float\n",
    "            The bandwidth parameter for kernel density estimation.\n",
    "        scaled_state_df : DataFrame\n",
    "            Scaled state data.\n",
    "        reward_df : DataFrame\n",
    "            DataFrame containing the reward values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Estimated reward values for each state.\n",
    "        \"\"\"\n",
    "        # Initialize reward estimator\n",
    "        r_pi_hat = est_r_pi(h_x)  # Instantiate the reward estimator\n",
    "        r_pi_hat.fit(scaled_state_df.values, reward_df.values.ravel())\n",
    "\n",
    "        r_pi_est = r_pi_hat(scaled_state_df.values)  # Estimate rewards\n",
    "\n",
    "        return r_pi_est\n",
    "\n",
    "    def estimation_reward_w(self, h_x, scaled_state_df, reward_df, w):\n",
    "        \"\"\"\n",
    "        Estimate the reward over a time window `w`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_x : float\n",
    "            The bandwidth parameter for kernel density estimation.\n",
    "        scaled_state_df : DataFrame\n",
    "            Scaled state data.\n",
    "        reward_df : DataFrame\n",
    "            DataFrame containing the reward values.\n",
    "        w : int\n",
    "            The time window over which to estimate the reward.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            Estimated reward values over the time window.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize time-windowed reward estimator\n",
    "        r_pi_w_hat = est_r_pi_w(h_x)  \n",
    "        r_pi_w_hat.fit(scaled_state_df.values, reward_df.values.ravel(), w)\n",
    "\n",
    "        r_pi_w_est = r_pi_w_hat(scaled_state_df.values)  # Estimate rewards over time window `w`\n",
    "\n",
    "        return r_pi_w_est\n",
    "\n",
    "    def compute_basis(self, h_x, scaled_state_df, scaled_next_x_df, order: int = 3):\n",
    "        \"\"\"\n",
    "        Compute the Chebyshev basis functions and their expectations for the next state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        h_x : float\n",
    "            The bandwidth parameter for kernel density estimation.\n",
    "        scaled_state_df : DataFrame\n",
    "            Scaled state data.\n",
    "        scaled_next_x_df : DataFrame\n",
    "            Scaled next state data.\n",
    "        order : int, optional\n",
    "            The order of the Chebyshev basis function expansion.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        DataFrame\n",
    "            DataFrame containing the basis functions.\n",
    "        dict\n",
    "            Dictionary of basis function definitions.\n",
    "        ndarray\n",
    "            Estimated expectations of the next state's basis functions.\n",
    "        \"\"\"\n",
    "\n",
    "        c_basis = ChebyshevBasis(order)  # Instantiate the Chebyshev basis object\n",
    "        basis_df, basis_dict = c_basis(scaled_state_df.values)  # Compute the basis functions\n",
    "\n",
    "        next_psi = BasisNextExpect(h_x)  # Instantiate the next-state basis expectation estimator\n",
    "        next_psi.fit(scaled_state_df.values, scaled_next_x_df.values)\n",
    "\n",
    "        # Estimate expectations for the next state\n",
    "        hat_psi_next = next_psi(scaled_state_df.values, c_basis)\n",
    "\n",
    "        return basis_df, basis_dict, hat_psi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0455da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HACTest:\n",
    "    \"\"\"\n",
    "    A class to perform statistical testing with HAC (Heteroscedasticity and Autocorrelation Consistent) standard errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta_df : pandas.DataFrame\n",
    "        The input DataFrame containing the data for testing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, theta_df):\n",
    "        \"\"\"\n",
    "        Initializes the HACTest with the input data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta_df : pandas.DataFrame\n",
    "            The data on which to perform the HAC test.\n",
    "        \"\"\"\n",
    "        self.theta_df = theta_df\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Perform HAC testing on the data with an automatically computed maximum lag.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        test_result : pandas.DataFrame\n",
    "            DataFrame containing formatted test statistics and results.\n",
    "        latex_result : str\n",
    "            LaTeX formatted table of test results for presentation.\n",
    "        original_result : tuple\n",
    "            Tuple of raw result dictionaries with numerical statistics.\n",
    "        \"\"\"\n",
    "        # Calculate the maximum lag using a rule of thumb based on the sample size\n",
    "        max_lag = int(4*((self.theta_df.shape[0]/100)** (1/3)))\n",
    "        \n",
    "        # Perform the HAC test and return the results\n",
    "        test_result, latex_result, original_result = self.test_hac(self.theta_df, max_lag)\n",
    "        \n",
    "        return test_result, latex_result, original_result\n",
    "    \n",
    "    def significance_stars(self, p_value):\n",
    "        \"\"\"\n",
    "        Assign significance stars based on p-value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        p_value : float\n",
    "            The p-value for statistical significance.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Stars representing the level of significance ('***' for p < 0.01, '**' for p < 0.05, '*' for p < 0.1).\n",
    "        \"\"\"\n",
    "        if p_value < 0.01:\n",
    "            return \"***\"\n",
    "        elif p_value < 0.05:\n",
    "            return \"**\"\n",
    "        elif p_value < 0.1:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"\"\n",
    "        \n",
    "    def compute_statistics(self, column_data: np.ndarray, max_lag: int):\n",
    "        \"\"\"\n",
    "        Fit a regression model and compute HAC-based statistics for a single column of data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        column_data : np.ndarray\n",
    "            Array containing the data for a single variable/column.\n",
    "        max_lag : int\n",
    "            Maximum lag to use for HAC standard errors.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        formatted_results : tuple\n",
    "            A tuple containing formatted strings for coefficient estimate, standard error, t-value, p-value, and significance stars.\n",
    "        original_results : dict\n",
    "            A dictionary containing the original numeric statistics.\n",
    "        \"\"\"\n",
    "        X = np.ones((len(column_data), 1))  # Constant term as the only explanatory variable\n",
    "        y = column_data  # Response variable\n",
    "        \n",
    "        # Fit the OLS model with HAC standard errors\n",
    "        model = sm.OLS(y, X).fit(cov_type='HAC', cov_kwds={'maxlags': max_lag})\n",
    "        \n",
    "        coef_estimate = model.params[0]  # Coefficient estimate\n",
    "        hac_std_error = model.bse[0]  # HAC standard error\n",
    "        \n",
    "        # Calculate the t-value and p-value\n",
    "        t_value = coef_estimate / hac_std_error\n",
    "        p_value = 2 * (1 - stats.norm.cdf(np.abs(t_value)))  # Two-sided p-value\n",
    "        \n",
    "        # Assign significance stars based on the p-value\n",
    "        stars = self.significance_stars(p_value)\n",
    "\n",
    "        # Format the results into a tuple for display\n",
    "        formatted_results = (\n",
    "            f\"{coef_estimate:.4g}\",\n",
    "            f\"{hac_std_error:.4g}\",\n",
    "            f\"{t_value:.4g}\",\n",
    "            f\"{p_value:.4g}\",\n",
    "            stars\n",
    "        )\n",
    "\n",
    "        # Store original numeric results in a dictionary\n",
    "        original_results = {\n",
    "            \"coef_estimate\": coef_estimate,\n",
    "            \"hac_std_error\": hac_std_error,\n",
    "            \"t_value\": t_value,\n",
    "            \"p_value\": p_value,\n",
    "            \"stars\": stars\n",
    "        }\n",
    "\n",
    "        return formatted_results, original_results\n",
    "    \n",
    "    def scientific_to_latex(self, value):\n",
    "        \"\"\"\n",
    "        Convert a numerical value to a LaTeX string in scientific notation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        value : float\n",
    "            The value to convert.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The LaTeX formatted string.\n",
    "        \"\"\"\n",
    "        if value == 0:\n",
    "            return \"$0$\"\n",
    "        else:\n",
    "            exp = int(np.floor(np.log10(np.abs(value))))\n",
    "            base = value / 10**exp\n",
    "            return f\"${base:.4f} \\\\times 10^{{{exp}}}$\"\n",
    "            \n",
    "    def test_hac(self, data, max_lag: int):\n",
    "        \"\"\"\n",
    "        Compute HAC standard errors and statistics for each column of data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : pandas.DataFrame\n",
    "            The input data with columns as variables.\n",
    "        max_lag : int\n",
    "            Maximum lag for HAC standard errors.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame containing the formatted test results.\n",
    "        latex_table : str\n",
    "            LaTeX formatted table for easy insertion into documents.\n",
    "        original_results : list\n",
    "            List of dictionaries with original numeric results.\n",
    "        \"\"\"\n",
    "        # Compute statistics for each column in parallel\n",
    "        results = Parallel(n_jobs=-1)(delayed(self.compute_statistics)(data.values[:, i], max_lag) for i in range(data.shape[1]))\n",
    "\n",
    "        # Unpack the results into formatted and original results\n",
    "        formatted_results, original_results = zip(*results)\n",
    "\n",
    "        # Convert results to a NumPy array\n",
    "        results_array = np.array(formatted_results)\n",
    "\n",
    "        # Create DataFrame of results\n",
    "        column_names = [\"Coefficient Estimate\", \"HAC Standard Error\", \"t-Value\", \"p-Value\", \"Significance\"]\n",
    "        df = pd.DataFrame(results_array, columns=column_names, index=data.columns)\n",
    "\n",
    "        # Convert numerical columns to LaTeX scientific notation\n",
    "        for col in [\"Coefficient Estimate\", \"HAC Standard Error\", \"t-Value\", \"p-Value\"]:\n",
    "            df[col] = df[col].astype(float).apply(self.scientific_to_latex)\n",
    "\n",
    "        # Create LaTeX table format\n",
    "        latex_table = df.to_latex(index=True, escape=False).replace(r'\\toprule', r'\\hline').replace(r'\\midrule', r'\\hline').replace(r'\\bottomrule', r'\\hline')\n",
    "\n",
    "        print(\"LaTeX formatted table:\")\n",
    "        print(latex_table)\n",
    "\n",
    "        return df, latex_table, original_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4790174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
